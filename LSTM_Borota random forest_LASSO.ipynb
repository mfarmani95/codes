{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import (KFold,\n",
    "                                     GridSearchCV,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     cross_validate,\n",
    "                                     \n",
    "                                     train_test_split)\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import metrics\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense,LSTM,Dropout,GRU,BatchNormalization\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import keras\n",
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer,Real,Categorical\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective ,plot_evaluations\n",
    "from skopt.utils import use_named_args\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "data=pd.read_csv(\"..................\")\n",
    "y=pd.DataFrame(data['observational'])\n",
    "x=df.drop(columns='rain')\n",
    "\n",
    "## feature selection_ boruta random forest feature selection method\n",
    "model = RandomForestRegressor()\n",
    "feat_selector = BorutaPy(model, n_estimators='auto', verbose=2)\n",
    "feat_selector.fit(x, y)\n",
    "boruta_ranking=pd.DataFrame({\"featre\":x.columns,\"Ranking\":list(feat_selector.ranking_)})\n",
    "trans=feat_selector.transform(X)\n",
    "\n",
    "## feature selection RFE \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg=LinearRegression()\n",
    "rfe_selector=RFE(reg,?) ## the ? determines number of features that we want to select\n",
    "rfe_selector.fit(X_aray, Y)\n",
    "\n",
    "md=pd.DataFrame({\"features\":x.columns, \"Ranking\":list(rfe_selector.ranking_)})\n",
    "md.plot.bar()\n",
    "plot.show\n",
    "\n",
    "## feature selection LASSO\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso=Lasso()\n",
    "lasso.fit(x,y)\n",
    "lasso.coef_\n",
    "## LSTM main\n",
    "day=? ## The number of lag-days. Some articles use lag-days for better prediction.\n",
    "vorodi=np.array(##selected features)\n",
    "main_input=list()\n",
    "for i in range(day,len(vorodi)):\n",
    "    main_input.append(vorodi[i-day:i])\n",
    "y=np.array(y)\n",
    "y_input=y[day:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(main_input, y_input, test_size=0.3)\n",
    "def lstm( learning_rate, num_LSTM_nodes, num_LSTM_nodes1, num_LSTM_nodes2,\n",
    "num_LSTM_nodes3, Drop_out, Drop_out1, Drop_out2, Drop_out3):\n",
    "# prepare model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(num_LSTM_nodes,\n",
    "    input_shape=(X_train.shape[1:]),\n",
    "    return_sequences=True,\n",
    "    ))\n",
    "    model.add(Dropout(Drop_out)) \n",
    "    
    "    model.add(LSTM(num_LSTM_nodes1,\n",
    "    return_sequences=True,\n",
    "    ))\n",
    "    model.add(Dropout(Drop_out1))\n",
    "    model.add(LSTM(num_LSTM_nodes2,\n",
    "    return_sequences=True,\n",
    "    ))\n",
    "    model.add(Dropout(Drop_out2))\n",
    "    model.add(LSTM(num_LSTM_nodes3, return_sequences=False))\n",
    "    model.add(Dropout(Drop_out3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1,\n",
    "    activation='relu'\n",
    "    ))\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile( loss='mse', optimizer= optimizer,\n",
    "    metrics=['accuracy'], ##or metrics=[[tf.keras.metrics.RootMeanSquaredError()]]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "## Hyperparametersâ€™ space\n",
    "dim_learning_rate= Real(\n",
    "low=1e-6 , high=1e-1, prior='log-uniform',name='learning_rate')\n",
    "dim_num_LSTM_nodes=Integer(\n",
    "low=10, high= 70 , name='num_LSTM_nodes')\n",
    "dim_num_LSTM_nodes1=Integer(\n",
    "low=10, high= 70 , name='num_LSTM_nodes1')\n",
    "dim_num_LSTM_nodes2=Integer(\n",
    "low=10, high= 70 , name='num_LSTM_nodes2')\n",
    "dim_num_LSTM_nodes3=Integer(\n",
    "low=10, high= 70 , name='num_LSTM_nodes3')\n",
    "dim_batch_size=Integer(\n",
    "low=100, high=2500, name='batch_size' )\n",
    "dim_drop_out= Real(\n",
    "low=1e-2 , high=5e-1, prior='uniform' ,name='drop_out')\n",
    "dim_drop_out1= Real(\n",
    "low=1e-2 , high=5e-1, prior='uniform',name='drop_out1')\n",
    "dim_drop_out2= Real(\n",
    "low=1e-2 , high=5e-1, prior='uniform',name='drop_out2')\n",
    "dim_drop_out3= Real(\n",
    "low=1e-2 , high=5e-1, prior='uniform',name='drop_out3')\n",
    "param_grid=[ dim_learning_rate,\n",
    "dim_num_LSTM_nodes,\n",
    "dim_num_LSTM_nodes1,\n",
    "dim_num_LSTM_nodes2,\n",
    "dim_num_LSTM_nodes3,\n",
    "dim_batch_size,\n",
    "dim_drop_out,\n",
    "dim_drop_out1,\n",
    "dim_drop_out2,\n",
    "dim_drop_out3,\n",
    "]\n",
    "\n",
    "## optimization\n",
    "## amalan kare optimize ro in function anjam mide\n",
    "@use_named_args(param_grid)\n",
    "def objectivy(learning_rate,\n",
    "    num_dense_layers,\n",
    "    num_dense_nodes,\n",
    "    activation,\n",
    "    num_LSTM_nodes\n",
    "    ):\n",
    "   \n",
    "    print('num_dense_layers:',num_dense_layers)\n",
    "  # print('num_dense_nodes:',num_dense_nodes)\n",
    "    print('activation:',activation)\n",
    "    print('num_LSTM_neural:', num_LSTM_nodes )\n",
    "    print()\n",
    "    \n",
    "    model=lstm(learning_rate= learning_rate,\n",
    "                     num_dense_layers= num_dense_layers,\n",
    "                     drop_out=drop_out\n",
    "                     Drop_out1=drop_out1\n",
    "                     drop_out2=drop_out2\n",
    "                     drop_out3=drop_out3\n",
    "                     num_dense_nodes= num_dense_nodes,\n",
    "                   #  activation= activation,\n",
    "                     num_LSTM_nodes=num_LSTM_nodes\n",
    "                     num_LSTM_nodes1=num_LSTM_nodes1,\n",
    "                     num_LSTM_nodes2=num_LSTM_nodes2,\n",
    "                     num_LSTM_nodes3=num_LSTM_nodes3,\n",
    "                     )\n",
    "    \n",
    "    learning_rate_reduction=ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                 patience=2,\n",
    "                                                 verbose=1,\n",
    "                                                 factor=0.5, \n",
    "                                                 min_lr=0.000001    \n",
    "                                                 )\n",
    "   \n",
    "    history=model.fit(x=X_train,\n",
    "                      y=y_train,\n",
    "                      epochs=4000, \n",
    "                      batch_size=batch_size, \n",
    "                      validation_split=0.2,\n",
    "                      callbacks=learning_rate_reduction \n",
    "                      )\n",
    "    \n",
    "    accuracy=history.history['val_loss'][-1]\n",
    "   \n",
    "    global best_accuracy\n",
    "    if accuracy < best_accuracy:\n",
    "      model.save(spath_best_model)\n",
    "      best_accuracy=accuracy\n",
    "    del model \n",
    "    
    "    return accuracy\n",
    "default_parameters=[.....,...,....,....]\n",
    "\n",
    "gp=gp_minimize(\n",
    "    objectivy,\n",
    "    param_grid,\n",
    "    x0=default_parameters,\n",
    "    acq_func=?,  ## this function should be selected based on the data \n",
    "    n_calls=100,\n",
    "    \n",
    "    \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
